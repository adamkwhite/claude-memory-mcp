name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run weekly on Sundays at 6 AM UTC
    - cron: '0 6 * * 0'

permissions:
  contents: read
  pull-requests: write

jobs:
  performance-tests:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio psutil
        pip install -e .
    
    - name: Generate test data
      run: |
        python scripts/generate_test_data.py --conversations 159 --clean
        
    - name: Cache test data
      uses: actions/cache@v3
      with:
        path: ~/claude-memory-test
        key: test-data-159-convs-v1
        
    - name: Run performance benchmarks
      run: |
        PYTHONPATH=. python -m pytest tests/test_performance_benchmarks.py -v --tb=short
        
    - name: Generate performance report
      run: |
        # Find latest results file
        RESULTS_FILE=$(ls -t benchmark_results/performance_results_*.json | head -1)
        python scripts/generate_performance_report.py "$RESULTS_FILE"
        
    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: docs/PERFORMANCE_BENCHMARKS.md
        
    - name: Upload benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results
        path: benchmark_results/
        
    - name: Check performance regression
      run: |
        # Extract key metrics from the latest run
        RESULTS_FILE=$(ls -t benchmark_results/performance_results_*.json | head -1)
        
        # Check if search time for 159 conversations exceeds 5 seconds
        SEARCH_TIME=$(python3 -c "
        import json
        with open('$RESULTS_FILE', 'r') as f:
            data = json.load(f)
        readme_results = [r for r in data['results'] if r['operation'] == 'readme_claim_validation']
        if readme_results:
            print(readme_results[0]['duration_seconds'])
        else:
            print('0')
        ")
        
        echo "Search time for 159 conversations: ${SEARCH_TIME}s"
        
        # Fail if search takes longer than 5 seconds
        if (( $(echo "$SEARCH_TIME > 5.0" | bc -l) )); then
          echo "‚ùå Performance regression: Search took ${SEARCH_TIME}s (> 5s threshold)"
          exit 1
        else
          echo "‚úÖ Performance check passed: ${SEARCH_TIME}s < 5s"
        fi
        
    - name: Comment performance results on PR
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          
          // Read the performance report
          const report = fs.readFileSync('docs/PERFORMANCE_BENCHMARKS.md', 'utf8');
          
          // Extract key metrics for comment
          const lines = report.split('\n');
          const summaryStart = lines.findIndex(l => l.includes('## Executive Summary'));
          const summaryEnd = lines.findIndex(l => l.includes('## README Claims Validation'));
          
          let summary = '';
          if (summaryStart >= 0 && summaryEnd >= 0) {
            summary = lines.slice(summaryStart, summaryEnd).join('\n');
          }
          
          const comment = `## üöÄ Performance Benchmark Results
          
          ${summary}
          
          **Full Report**: See the uploaded artifact for complete details.
          
          ---
          *Automated performance check completed* ‚úÖ`;
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

  performance-comparison:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Fetch full history for comparison
        
    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio psutil
        pip install -e .
        
    - name: Run baseline performance (main branch)
      run: |
        git checkout main
        python scripts/generate_test_data.py --conversations 159 --clean --storage-path ~/baseline-test
        PYTHONPATH=. python -m pytest tests/test_performance_benchmarks.py::TestOverallPerformance::test_readme_claims_validation -v --tb=short
        # Rename results
        mkdir -p baseline_results
        mv benchmark_results/* baseline_results/ 2>/dev/null || true
        
    - name: Run current performance (PR branch)
      run: |
        git checkout ${{ github.event.pull_request.head.sha }}
        python scripts/generate_test_data.py --conversations 159 --clean --storage-path ~/current-test  
        PYTHONPATH=. python -m pytest tests/test_performance_benchmarks.py::TestOverallPerformance::test_readme_claims_validation -v --tb=short
        
    - name: Compare performance
      run: |
        python3 -c "
        import json
        import os
        
        # Load baseline results
        baseline_files = [f for f in os.listdir('baseline_results') if f.endswith('.json')]
        current_files = [f for f in os.listdir('benchmark_results') if f.endswith('.json')]
        
        if baseline_files and current_files:
            with open(f'baseline_results/{baseline_files[0]}', 'r') as f:
                baseline = json.load(f)
            with open(f'benchmark_results/{current_files[0]}', 'r') as f:
                current = json.load(f)
                
            # Extract search times
            baseline_time = next((r['duration_seconds'] for r in baseline['results'] if r['operation'] == 'readme_claim_validation'), 0)
            current_time = next((r['duration_seconds'] for r in current['results'] if r['operation'] == 'readme_claim_validation'), 0)
            
            if baseline_time > 0:
                change = ((current_time - baseline_time) / baseline_time) * 100
                print(f'Performance change: {change:+.1f}%')
                print(f'Baseline: {baseline_time:.3f}s ‚Üí Current: {current_time:.3f}s')
                
                # Check for significant regression (>10%)
                if change > 10:
                    print(f'‚ùå Performance regression detected: {change:+.1f}% slower')
                    exit(1)
                elif change < -10:
                    print(f'üöÄ Performance improvement: {change:+.1f}% faster')
                else:
                    print(f'‚úÖ Performance stable: {change:+.1f}% change')
            else:
                print('No baseline data available for comparison')
        else:
            print('Missing performance data files')
        "